# -*- coding: utf-8 -*-
"""Trabalho3_MIWRS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NoanRz0TMA7lIoXKzaSN80N0gEcQR9dR

Chaves API/Tokens:
* API Key : bTqPE6hXGC48rG75ZgVe1O6pF
* API Key Secret: 1fcBpK3ztx76nq6g411t12GWdXgp1u4hSUqZJTByKgrmiteRZv

* BeaberToken:AAAAAAAAAAAAAAAAAAAAAG7o0wEAAAAAyq8C40ZKCyQrWxG1Ks1l530OAxs%3DyKGvtGA1Hv75rXZMDvZTNn5qpzDmcjHuv2LFTMyDvtZAmpMuTW

* Access Token: 1773471593223655424-UiSINXUqyA9rxXaz9k13kndeXLAaHW

* Access Token Secret : llRi53liFzasT2ToRgz3fdsAJzHJn1y461JkBNWW8C7cy

Configuração Inicial
"""

!pip install tweepy pandas matplotlib seaborn wordcloud transformers torch

"""Coleta de Dados"""

import requests
import json
import time
import os
import pandas as pd

BEARER_TOKEN = "AAAAAAAAAAAAAAAAAAAAAG7o0wEAAAAA8Uf0sexO5dfDaw4SgcWBlR6iUBE%3DtvKMzaxfin4i4ToBYKXep5imiGaL9gtKuJOPeFt0tEgP4jESIk"

headers = {"Authorization": f"Bearer {BEARER_TOKEN}"}

params_base = {
    # ATENÇÃO AQUI - FILTRO POR BRASIL E PORTUGUÊS
    "query": '(SUS OR "saúde pública" OR vacinação) lang:pt',
    "max_results": 100,
    "tweet.fields": "created_at,public_metrics,lang,geo"
}

if os.path.exists("tweets_saude_brasil.json"):
    with open("tweets_saude_brasil.json", "r", encoding="utf-8") as f:
        all_tweets = json.load(f).get("data", [])
else:
    all_tweets = []

next_token = None

for i in range(3):
    params = params_base.copy()
    if next_token:
        params["next_token"] = next_token

    response = requests.get(
        "https://api.twitter.com/2/tweets/search/recent",
        headers=headers,
        params=params
    )

    if response.status_code == 200:
        data = response.json()
        new_tweets = data.get("data", [])
        meta = data.get("meta", {})
        next_token = meta.get("next_token")

        print(f"Página {i+1}: {len(new_tweets)} novos tweets coletados.")
        all_tweets.extend(new_tweets)

        with open("tweets_saude_brasil.json", "w", encoding="utf-8") as f:
            json.dump({"data": all_tweets}, f, indent=2, ensure_ascii=False)

        if not next_token:
            print("Não há mais páginas de tweets disponíveis.")
            break

    elif response.status_code == 429:
        reset_time = int(response.headers.get("x-rate-limit-reset", time.time() + 60))
        sleep_time = max(reset_time - time.time(), 0)
        print(f"Limite atingido. Aguardando {int(sleep_time)} segundos até reset...")
        time.sleep(sleep_time)
    else:
        print(f"Erro {response.status_code}: {response.text}")
        break

    time.sleep(2)

import json
import pandas as pd

data = json.load(open('tweets_saude_brasil_1.json'))
df_tweets = pd.json_normalize(data['data'])
print(df_tweets)

"""Pré-processamento"""

import re
import nltk
from nltk.corpus import stopwords

nltk.download('stopwords')

stop_words = set(stopwords.words('portuguese'))

def limpar_texto(texto):
  texto = re.sub(r'http\S+', '', texto) # remove links
  texto = re.sub(r'@[^\s]+', '', texto) # remove menções
  texto = re.sub(r'#', '', texto) # remove hashtags (mas mantém o texto)
  texto = re.sub(r'[^\w\s]', '',texto) # remove caracteres especiais
  texto = texto.lower() # converte em minúsculas
  texto = ' '.join([word for word in texto.split() if word not in stop_words])
  return texto

df_tweets['texto_limpo'] = df_tweets['text'].apply(limpar_texto)
df_tweets.head()

"""Análise exploratória"""

import json

# Carrega os dados do arquivo JSON
with open('tweets_saude_brasil.json', 'r', encoding='utf-8') as f:
    dados = json.load(f)

# Obtém a lista de tweets
tweets = dados.get('data', [])

# Calcula as métricas
quantidade_registros = len(tweets)
quantidade_frases = sum(len(tweet['text'].split('.')) for tweet in tweets)
tamanho_medio = sum(len(tweet['text']) for tweet in tweets) / quantidade_registros if quantidade_registros > 0 else 0

# Exibe os resultados
print(f"Quantidade de registros: {quantidade_registros}")
print(f"Quantidade de frases: {quantidade_frases}")
print(f"Tamanho médio das frases: {tamanho_medio:.2f} caracteres")

# Média de engajamento (likes, retweets, replies)
engajamento = {
    'avg_retweets': sum(tweet.get('public_metrics', {}).get('retweet_count', 0) for tweet in tweets) / max(1, len(tweets)),
    'avg_replies': sum(tweet.get('public_metrics', {}).get('reply_count', 0) for tweet in tweets) / max(1, len(tweets))
}
# Contagem de tweets com mídia (imagens/vídeos)
tweets_com_midia = sum(1 for tweet in tweets if 'media' in tweet.get('attachments', {}))
percentual_midia = (tweets_com_midia / max(1, len(tweets))) * 100

# Exibição formatada
print("\n ANÁLISE DE ENGAJAMENTO")
print("----------------------------------")
print(f" Média de retweets: {engajamento['avg_retweets']:.1f}")
print(f" Média de replies: {engajamento['avg_replies']:.1f}")
print("----------------------------------")
print(f" Total de tweets analisados: {len(tweets)}")

"""Classificação de Sentimentos com HuggingFace Transformers"""

from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline
import torch

# Usar GPU se disponível
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Modelo pré-treinado
modelo = "cardiffnlp/twitter-roberta-base-sentiment"
tokenizer = AutoTokenizer.from_pretrained(modelo)
model = AutoModelForSequenceClassification.from_pretrained(modelo).to(device)

sentiment_pipeline = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)

# Aplica a análise
df_tweets['sentimento'] = df_tweets['texto_limpo'].apply(lambda x: sentiment_pipeline(x[:512])[0]['label']) # limitando texto em 512 caracteres

df_tweets

"""Visualização de Resultados"""

# Gráfico de barras dos sentimentos
import matplotlib.pyplot as plt
import seaborn as sns

sns.set_style("whitegrid")
plt.figure(figsize=(8, 5))
sns.countplot(x='sentimento', data=df_tweets, order=df_tweets['sentimento'].value_counts().index)
plt.title("Distribuição de sentimentos")
plt.xlabel("Sentimento")
plt.ylabel("Contagem")
plt.show()